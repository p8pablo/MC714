"""
Sistema de an√°lise e relat√≥rios de m√©tricas
"""
from typing import Dict, List, Any
import json
from datetime import datetime


class MetricsAnalyzer:
    def __init__(self):
        self.simulation_results = []
    
    def add_simulation_result(self, result: Dict[str, Any], experiment_name: str = ""):
        """Adiciona resultado de uma simula√ß√£o"""
        result["experiment_name"] = experiment_name
        result["timestamp"] = datetime.now().isoformat()
        self.simulation_results.append(result)
    
    def compare_policies(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Compara performance entre diferentes pol√≠ticas"""
        comparison = {
            "policies_compared": [],
            "metrics_comparison": {},
            "best_policy": {},
            "summary": {}
        }
        
        for result in results:
            policy = result["load_balancer_stats"]["policy"]
            comparison["policies_compared"].append(policy)
            
            system_metrics = result["system_metrics"]
            
            if policy not in comparison["metrics_comparison"]:
                comparison["metrics_comparison"][policy] = {}
            
            comparison["metrics_comparison"][policy].update({
                "throughput": system_metrics["system_throughput"],
                "avg_response_time": system_metrics["avg_response_time"],
                "avg_waiting_time": system_metrics["avg_waiting_time"],
                "utilization": system_metrics["system_utilization"],
                "response_time_std": system_metrics["response_time_std"],
                "total_processed": system_metrics["total_processed"]
            })
        
        # Determina melhor pol√≠tica para cada m√©trica
        comparison["best_policy"] = self._find_best_policies(comparison["metrics_comparison"])
        
        # Resumo geral
        comparison["summary"] = self._generate_policy_summary(comparison["metrics_comparison"])
        
        return comparison
    
    def _find_best_policies(self, metrics: Dict[str, Dict[str, float]]) -> Dict[str, str]:
        """Encontra a melhor pol√≠tica para cada m√©trica"""
        best = {}
        
        # Throughput - maior √© melhor
        max_throughput = max(metrics[policy]["throughput"] for policy in metrics)
        for policy in metrics:
            if metrics[policy]["throughput"] == max_throughput:
                best["throughput"] = policy
                break
        
        # Response time - menor √© melhor
        min_response_time = min(metrics[policy]["avg_response_time"] for policy in metrics)
        for policy in metrics:
            if metrics[policy]["avg_response_time"] == min_response_time:
                best["avg_response_time"] = policy
                break
        
        # Waiting time - menor √© melhor
        min_waiting_time = min(metrics[policy]["avg_waiting_time"] for policy in metrics)
        for policy in metrics:
            if metrics[policy]["avg_waiting_time"] == min_waiting_time:
                best["avg_waiting_time"] = policy
                break
        
        # Utilization - maior √© melhor (mas n√£o muito pr√≥ximo de 1)
        utilizacoes = {policy: metrics[policy]["utilization"] for policy in metrics}
        best_util_policy = max(utilizacoes, key=utilizacoes.get)
        best["utilization"] = best_util_policy
        
        return best
    
    def _generate_policy_summary(self, metrics: Dict[str, Dict[str, float]]) -> Dict[str, Any]:
        """Gera resumo das pol√≠ticas"""
        summary = {}
        
        for policy in metrics:
            policy_metrics = metrics[policy]
            
            # Score baseado em m√∫ltiplas m√©tricas (0-100)
            # Normaliza m√©tricas e calcula score ponderado
            throughput_score = min(policy_metrics["throughput"] * 10, 100)  # Assume max ~10 req/s
            response_score = max(0, 100 - policy_metrics["avg_response_time"] * 20)  # Penaliza response time alto
            waiting_score = max(0, 100 - policy_metrics["avg_waiting_time"] * 30)  # Penaliza waiting time alto
            util_score = policy_metrics["utilization"] * 100  # Utiliza√ß√£o como percentual
            
            overall_score = (throughput_score * 0.3 + response_score * 0.3 + 
                           waiting_score * 0.2 + util_score * 0.2)
            
            summary[policy] = {
                "overall_score": round(overall_score, 2),
                "strengths": [],
                "weaknesses": [],
                "recommended_for": ""
            }
            
            # Identifica pontos fortes e fracos
            if policy_metrics["throughput"] > 2.0:
                summary[policy]["strengths"].append("Alto throughput")
            if policy_metrics["avg_response_time"] < 1.0:
                summary[policy]["strengths"].append("Baixo tempo de resposta")
            if policy_metrics["avg_waiting_time"] < 0.5:
                summary[policy]["strengths"].append("Baixo tempo de espera")
            if policy_metrics["utilization"] > 0.7:
                summary[policy]["strengths"].append("Boa utiliza√ß√£o de recursos")
            
            if policy_metrics["throughput"] < 1.5:
                summary[policy]["weaknesses"].append("Throughput baixo")
            if policy_metrics["avg_response_time"] > 2.0:
                summary[policy]["weaknesses"].append("Tempo de resposta alto")
            if policy_metrics["utilization"] < 0.5:
                summary[policy]["weaknesses"].append("Subutiliza√ß√£o de recursos")
            
            # Recomenda√ß√µes de uso
            if policy == "random":
                summary[policy]["recommended_for"] = "Cargas uniformes e simples implementa√ß√£o"
            elif policy == "round_robin":
                summary[policy]["recommended_for"] = "Distribui√ß√£o equitativa e servidores homog√™neos"
            elif policy == "shortest_queue":
                summary[policy]["recommended_for"] = "Cargas heterog√™neas e otimiza√ß√£o de lat√™ncia"
        
        return summary
    
    def analyze_traffic_patterns(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analisa performance sob diferentes padr√µes de tr√°fego"""
        analysis = {
            "patterns_analyzed": [],
            "pattern_performance": {},
            "recommendations": {}
        }
        
        for result in results:
            pattern = result["traffic_stats"]["pattern"]
            analysis["patterns_analyzed"].append(pattern)
            
            if pattern not in analysis["pattern_performance"]:
                analysis["pattern_performance"][pattern] = []
            
            analysis["pattern_performance"][pattern].append({
                "policy": result["load_balancer_stats"]["policy"],
                "throughput": result["system_metrics"]["system_throughput"],
                "response_time": result["system_metrics"]["avg_response_time"],
                "utilization": result["system_metrics"]["system_utilization"]
            })
        
        # Gera recomenda√ß√µes para cada padr√£o
        for pattern in analysis["pattern_performance"]:
            pattern_data = analysis["pattern_performance"][pattern]
            best_policy = max(pattern_data, key=lambda x: x["throughput"])
            
            analysis["recommendations"][pattern] = {
                "best_policy": best_policy["policy"],
                "reason": f"Melhor throughput ({best_policy['throughput']:.2f} req/s) para padr√£o {pattern}"
            }
        
        return analysis
    
    def generate_report(self, output_file: str = "simulation_report.json"):
        """Gera relat√≥rio completo em JSON"""
        if not self.simulation_results:
            return {"error": "Nenhum resultado de simula√ß√£o dispon√≠vel"}
        
        report = {
            "report_generated": datetime.now().isoformat(),
            "total_simulations": len(self.simulation_results),
            "simulation_results": self.simulation_results
        }
        
        # An√°lise comparativa se houver m√∫ltiplas simula√ß√µes
        if len(self.simulation_results) > 1:
            report["policy_comparison"] = self.compare_policies(self.simulation_results)
            report["traffic_analysis"] = self.analyze_traffic_patterns(self.simulation_results)
        
        # Salva arquivo
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"Relat√≥rio salvo em: {output_file}")
        return report
    
    def print_summary(self, result: Dict[str, Any]):
        """Imprime resumo da simula√ß√£o na tela"""
        print(f"\n{'='*80}")
        print(f"RESUMO DA SIMULA√á√ÉO")
        print(f"{'='*80}")
        
        # Informa√ß√µes gerais
        print(f"Tempo de simula√ß√£o: {result['simulation_time']:.2f}s")
        print(f"Pol√≠tica: {result['load_balancer_stats']['policy']}")
        print(f"Padr√£o de tr√°fego: {result['traffic_stats']['pattern']}")
        
        # M√©tricas do sistema
        sys_metrics = result["system_metrics"]
        print(f"\nüìä M√âTRICAS DO SISTEMA:")
        print(f"  ‚Ä¢ Throughput: {sys_metrics['system_throughput']:.3f} req/s")
        print(f"  ‚Ä¢ Tempo m√©dio de resposta: {sys_metrics['avg_response_time']:.3f}s")
        print(f"  ‚Ä¢ Tempo m√©dio de espera: {sys_metrics['avg_waiting_time']:.3f}s")
        print(f"  ‚Ä¢ Utiliza√ß√£o do sistema: {sys_metrics['system_utilization']:.1%}")
        print(f"  ‚Ä¢ Total processado: {sys_metrics['total_processed']} requisi√ß√µes")
        
        # M√©tricas dos servidores
        print(f"\nüñ•Ô∏è  M√âTRICAS DOS SERVIDORES:")
        for server_metric in result["server_metrics"]:
            print(f"  Server {server_metric['server_id']}:")
            print(f"    - Requisi√ß√µes processadas: {server_metric['processed_count']}")
            print(f"    - Throughput: {server_metric['throughput']:.3f} req/s")
            print(f"    - Tempo m√©dio de resposta: {server_metric['avg_response_time']:.3f}s")
        
        # Distribui√ß√£o de carga
        lb_stats = result["load_balancer_stats"]
        print(f"\n‚öñÔ∏è  DISTRIBUI√á√ÉO DE CARGA:")
        print(f"  ‚Ä¢ Pol√≠tica: {lb_stats['policy']}")
        print(f"  ‚Ä¢ Requisi√ß√µes distribu√≠das: {lb_stats['total_requests_distributed']}")
        print(f"  ‚Ä¢ Distribui√ß√£o por servidor:")
        for server_id, count in lb_stats['requests_per_server'].items():
            percentage = (count / lb_stats['total_requests_distributed']) * 100 if lb_stats['total_requests_distributed'] > 0 else 0
            print(f"    - Server {server_id}: {count} ({percentage:.1f}%)")
        print(f"  ‚Ä¢ Vari√¢ncia da distribui√ß√£o: {lb_stats['distribution_variance']:.3f}")
        
        print(f"{'='*80}")
    
    def clear_results(self):
        """Limpa resultados armazenados"""
        self.simulation_results = []